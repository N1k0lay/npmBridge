# Verdaccio Replication Service Architecture

## 1. Goals & Constraints

- Централизовать управление двумя Verdaccio-реестрами (онлайн-прокси и изолированная сеть) и сделать процессы прозрачными.
- Минимизировать ручные SSH-операции и запуск bash-скриптов (`p_pnpm_repo_update.sh`, `p_pnpm_update_modif_pkg.sh`, `trans_npm2.py`).
- Обеспечить прогнозируемую репликацию: после принудительного обновления кэша автоматически собирать дифф и доставлять его в офлайн-сеть.
- Предоставить web-интерфейс и API: статистика пакетов, история запусков, контроль задач, возможность инициировать обновление.
- Эксплуатировать существующий Linux-сервер с Verdaccio без дополнительного объектного хранилища: все снапшоты и артефакты лежат в локальном `storage`/`snapshots` каталоге.
- Авторизация/аутентификация не требуются (внутренний сегмент), уведомления и телеметрия не нужны.
- В офлайн-сеть переносится только один архив с пакетами (tar/zst); подписи и доп. метаданные передаются внутри архива при необходимости.
- Масштабирование горизонтально не требуется, решение ориентировано на один инстанс Verdaccio.

## 2. Существующие рабочие процессы (as-is)

1. **Кеширование**: Verdaccio в интернете выступает прокси, автоматически скачивая пакеты при установке.
2. **Принудительное обновление**: `p_pnpm_repo_update.sh` перечисляет все пакеты в `storage` и заново устанавливает свежие версии во временные директории.
3. **Частичное обновление**: `p_pnpm_update_modif_pkg.sh` переустанавливает пакеты, изменённые за последние N минут (по умолчанию ~2 дня).
4. **Подготовка диффа**: `trans_npm2.py` (или `frozen_storage.sh`) копирует новые/обновлённые файлы из «текущего» `storage` в отдельный `diff_dir`, чтобы перенести их офлайн.
5. **Ремонт повреждённых архивов**: вспомогательные скрипты (`check_broken_tar.sh`, `reinstall_brocken_arc.sh`) работают вручную через список `broken.txt`.

Проблемы: отсутствует наблюдаемость (кто и когда запускал), нельзя отслеживать прогресс, нет API, ручная сборка артефактов для изолированной сети, нет гарантий, что принудительное обновление завершилось до старта копирования.

## 3. Целевое решение (to-be)

Лёгкий сервис, расширяющий Verdaccio без лишней инфраструктуры:

- **Web/API слой**: FastAPI (Python) + фоновые задачи, фронтенд на React (Vite). UI доступен только внутри сети, без авторизации.
- **Metadata Collector**: периодически рекурсивно проходит `storage` (аналогично `p_pnpm_repo_update.sh`, который извлекает имена пакетов из структуры каталогов), считывает verdaccio-метаданные (`storage/<pkg>/package.json`, если файл присутствует) либо распаковывает заголовок `.tgz`, чтобы получить версии/размеры, и сохраняет агрегированную статистику в локальные JSON/SQLite файлы.
- **Job Orchestrator**: очередь задач на основе встроенных фоновых workers FastAPI + APScheduler или Celery/Redis (если понадобится больше параллельности). Задачи: `full_cache_refresh`, `delta_refresh`, `diff_build`, `broken_scan`.
- **Worker Runner**: адаптеры, которые вызывают существующие bash-скрипты или Python-обёртки (`jobs/full_refresh.py`) и транслируют stdout/stderr в лог.
- **Broken Archive Monitor**: модуль, который автоматически запускает `broken_scan` (аналог `check_broken_tar.sh`), фиксирует список повреждённых архивов, удаляет их и инициирует переустановку версий через pnpm/npm (повторяя логику `reinstall_brocken_arc.sh`). После переустановки архив проверяется ещё раз и помечается как восстановленный.
- **Artifact Builder**: после полного обновления сравнивает «текущий» `storage` с последним frozen snapshot (директория на том же диске), упаковывает только изменённые пакеты в `diff-YYYYMMDDHHmm.tar.zst`. Внутрь архива кладётся `manifest.json` (список пакетов/версий/хэшей) и README с инструкцией.
- **State store**: минимальный каталог `state/` с SQLite (`state/data.sqlite`) и вспомогательными файлами логов. Никаких MinIO/S3.

## 4. Основные потоки данных

1. **Triggered Cache Refresh**
   - Пользователь нажимает «Обновить всё» в UI или вызывает `POST /jobs/full-refresh`.
   - Оркестратор ставит задачу, Worker запускает переустановку пакетов (через pnpm CLI). Прогресс пишется в лог и state файл.
   - По завершении фиксируются статус, длительность, список затронутых пакетов.
2. **Automatic Delta Refresh**
   - Периодическая задача (например, раз в 12 часов) переустанавливает пакеты, изменённые за последние N минут, используя существующий скрипт.
3. **Diff Packaging**
   - После полного обновления делается snapshot (копия метаданных файлов) и запускается сравнение с предыдущим snapshot.
   - Попавшие в diff файлы копируются в staging-директорию и сразу упаковываются в единый `diff-*.tar.zst` внутри `artifacts/` на локальном диске.
4. **Offline Apply**
   - Архив переносится физически (диск/USB) или по защищённому каналу.
   - В изолированной сети администратор вручную распаковывает архив в каталог `storage`, проверяет sha512 по `manifest.json` и перезапускает Verdaccio.
5. **Broken Archive Repair**
   - Планировщик периодически запускает `broken_scan` (полнопроходный `tar -tzf` для всех `.tgz`). Список битых архивов сохраняется в SQLite (`broken_archives` таблица) и лог-файл.
   - Для каждого архива сервис удаляет файл, переустанавливает нужный пакет/версию во временную директорию через npm/pnpm, ожидает пока Verdaccio пересоздаст `.tgz`, затем повторно проверяет архив и обновляет статус (repaired/failed).

## 5. API Набросок

- `GET /packages?limit=...` — агрегированная статистика (число версий, дата обновления, размер каталога).
- `GET /packages/{name}` — подробности конкретного пакета и история попадания в архивы.
- `GET /jobs` / `GET /jobs/{id}` — список задач и их статусы.
- `POST /jobs/full-refresh` — запуск полного обновления (параметры: лимит параллелизма, dry-run).
- `POST /jobs/delta-refresh?minutes=2880` — запуск частичного обновления.
- `POST /jobs/diff-build?baseline=<snapshot-id>` — принудительная сборка архива на основе выбранного snapshot.
- `GET /artifacts` — список доступных архивов с путём на диске и контрольной суммой.
- `GET /broken-archives` / `POST /jobs/broken-rescan` — просмотр повреждённых архивов и принудительный запуск проверки/ремонта.
- `GET /health` — проверка готовности сервиса (используется UI).

## 6. Data Persistence Options

Варианты хранения состояния без полноценной СУБД:

- **Только файлы (JSON/YAML)**: каждая задача пишет отчёт в `state/jobs/<id>.json`, агрегированные метрики лежат в `state/packages.json`. Плюсы — отсутствие СУБД; минусы — сложнее делать фильтрацию, нужны блокировки при одновременной записи.
- **Встроенная SQLite**: один файл `state/data.sqlite` хранит таблицы `jobs`, `packages`, `artifacts`. Плюсы — транзакционность, простые запросы, нет отдельного сервера; минус — потребность в ORM/миграциях.

Решение: использовать SQLite как постоянное хранилище состояния (файл `state/data.sqlite`). Это минимальная БД, не требующая отдельного сервера, и она покрывает все потребности по историям задач и пакетам.

## 7. Brick-by-brick Implementation Plan

1. **Foundation**
   - Монорепо: `backend/` (FastAPI + SQLite + APScheduler), `frontend/` (React + Vite), `jobs/` (скрипты).
   - Docker Compose: Verdaccio, backend, frontend (статический nginx), Redis *опционально* (если понадобится очередь). По умолчанию backend работает без Redis.
2. **Job Runner**
   - Обернуть `p_pnpm_repo_update.sh` и `p_pnpm_update_modif_pkg.sh` в Python-классы с контролем параллелизма, логами и ограничением по CPU.
   - Хранить stdout/stderr задач в `logs/<job-id>.log`, ссылки на лог выдавать через API.
3. **Metadata Collector**
   - Раз в N минут сканировать `storage`, вычислять размеры, mtimes, sha512. Результат складывать в SQLite (или JSON) и кэшировать в памяти для UI.
4. **Diff Builder**
   - Переиспользовать код `trans_npm2.py`, но добавить стадию упаковки в tar/zst и генерацию `manifest.json` внутри архива.
   - Сохранить frozen snapshot (только структура + хэши) в `snapshots/<timestamp>/manifest.json`.
5. **Broken Archive Automation**
   - Реализовать Python-обёртку для `check_broken_tar.sh`/`reinstall_brocken_arc.sh`: параллельная проверка tar, запись результатов в SQLite, переустановка конкретных пакетов/версий и повторная валидация.
   - API/Frontend должны показывать список повреждённых архивов, статус переустановки и логи.
6. **Frontend**
   - React SPA с разделами: Dashboard, Packages, Jobs, Artifacts. Общение через REST, без авторизации.

## 8. Offline Pipeline Details

- **Baseline snapshot**: после успешного full refresh сохраняется список файлов/хэшей в `snapshots/<timestamp>/manifest.json` (на том же диске).
- **Diff computation**: manifest сравнивается с последним snapshot, отобранные файлы копируются в временный каталог `diff_work/` и сразу архивируются.
- **Archive layout**: внутри `diff-*.tar.zst` находятся реальная структура пакетов (`storage/<scope>/<pkg>/<version>/...`) и `manifest.json`. Никаких внешних зависимостей, только одного архива достаточно для переноса.
- **Integrity**: manifest внутри архива содержит sha512 каждого файла; проверка выполняется скриптом на офлайн-площадке перед копированием.

## 9. Deployment View

- **Online Zone**: Linux-сервер с Verdaccio, FastAPI backend, React frontend (статические файлы), SQLite файл в каталоге `state/`. По желанию — Redis для очередей.
- **Transfer Media**: физический носитель или разовая передача по сети. Никаких внешних хранилищ, архивы лежат в `artifacts/` рядом со `storage`.
- **Offline Zone**: Verdaccio. Пользователь вручную копирует архив, распаковывает его в `storage`, делает бэкап при необходимости и перезапускает Verdaccio.

## 10. Roadmap & Next Steps

1. Подтвердить выбор FastAPI + React + SQLite и описать структуру монорепо.
2. Реализовать API-слой с сущностями `Job`, `Package`, `Artifact` (SQLite) и эндпоинтами чтения/старта задач.
3. Инкапсулировать существующие bash-скрипты в Python-обёртки с контролем логов и таймаутов.
4. Добавить сбор статистики `storage` и базовый дашборд (React) без авторизации.
5. Реализовать snapshot + diff builder, генерацию архива и CLI для офлайн-применения.
6. Отладить процесс переноса архива на тестовом окружении, обновить README и инструкции.

## 11. Открытые вопросы

- Нужны ли дополнительные индексы/агрегации в SQLite (например, для быстрых фильтров по broken архивам)?
- Какие гарантии целостности нужны при переносе архива (достаточно ли sha512 внутри manifest)?
- Какой периодичности обновления (delta/forced) достаточно для бизнеса?
- Нужны ли дополнительные инструменты мониторинга дискового пространства `storage`?
